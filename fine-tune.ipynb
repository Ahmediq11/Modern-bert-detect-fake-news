{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ 1. Environment Setup & Library Installation\n",
        "\n",
        "This initial block handles the setup of our Python environment. We need to install several key libraries to build our sentiment analysis model.\n",
        "\n",
        "- **`transformers`**: The core Hugging Face library providing the ModernBERT model and the `Trainer` API.\n",
        "- **`accelerate`**: A companion library that optimizes PyTorch training across different hardware configurations.\n",
        "- **`datasets`**: Used to efficiently handle and preprocess our data, especially for integration with the `Trainer`.\n",
        "- **`bertviz` & `umap-learn`**: Visualization tools for inspecting model internals (optional but good practice).\n",
        "- **`seaborn`**: A powerful library for creating informative statistical visualizations like heatmaps and boxplots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U transformers\n",
        "# !pip install -U accelerate\n",
        "# !pip install -U datasets\n",
        "# !pip install -U bertviz\n",
        "# !pip install -U umap-learn\n",
        "# !pip install seaborn --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• 2. Data Loading & Initial Inspection\n",
        "\n",
        "We begin by loading our dataset using the `pandas` library, a staple for data manipulation in Python. The data is a CSV file of tweets hosted on GitHub.\n",
        "\n",
        "After loading, we perform a quick check-up:\n",
        "- **`df.info()`**: Gives a summary of the DataFrame, including data types and non-null counts.\n",
        "- **`df.isnull().sum()`**: Checks for any missing values, a critical data cleaning step.\n",
        "- **`df['label'].value_counts()`**: Shows the distribution of our target classes. This step is crucial as it will reveal if our dataset is imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/twitter_multi_class_sentiment.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["df.info()\n", "df.isnull().sum()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["df['label'].value_counts()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öñÔ∏è 3. Data Balancing via Undersampling\n",
        "\n",
        "Our initial inspection shows a significant **class imbalance** (the 'joy' class is much larger than others). A model trained on this data would be biased. To fix this, we perform **undersampling**.\n",
        "\n",
        "1. **Rename Class**: We rename `'joy'` to `'Normal'` for clarity.\n",
        "2. **Separate Classes**: Each emotional category is isolated into its own DataFrame.\n",
        "3. **Resample**: We use `resample` from `scikit-learn` to randomly select 500 samples from each class. This ensures every emotion has an equal number of training examples.\n",
        "4. **Concatenate**: The newly balanced DataFrames are combined into a single, balanced dataset.\n",
        "\n",
        "This creates a fair dataset for training our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['label_name'].replace('joy', 'Normal', inplace=True)\n",
        "display(df['label_name'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df['label_name'] == 'Normal']\n",
        "df_sadness = df[df['label_name'] == 'sadness']\n",
        "df_anger = df[df['label_name'] == 'anger']\n",
        "df_fear = df[df['label_name'] == 'fear']\n",
        "df_love = df[df['label_name'] == 'love']\n",
        "df_surprise = df[df['label_name'] == 'surprise']\n",
        "\n",
        "# Undersample majority class to 500\n",
        "df_majority_undersampled = resample(df_majority,\n",
        "                                    replace=False,    # sample without replacement\n",
        "                                    n_samples=500,     # to match minority class\n",
        "                                    random_state=42) # reproducible results\n",
        "\n",
        "df_sadness_undersampled = resample(df_sadness,\n",
        "                                    replace=False,    # sample without replacement\n",
        "                                    n_samples=500,     # to match minority class\n",
        "                                    random_state=42) # reproducible results\n",
        "\n",
        "df_anger_undersampled = resample(df_anger,\n",
        "                                    replace=False,    # sample without replacement\n",
        "                                    n_samples=500,     # to match minority class\n",
        "                                    random_state=42) # reproducible results\n",
        "\n",
        "df_fear_undersampled = resample(df_fear,\n",
        "                                    replace=False,    # sample without replacement\n",
        "                                    n_samples=500,     # to match minority class\n",
        "                                    random_state=42) # reproducible results\n",
        "\n",
        "df_love_undersampled = resample(df_love,\n",
        "                                    replace=False,    # sample without replacement\n",
        "                                    n_samples=500,     # to match minority class\n",
        "                                    random_state=42) # reproducible results\n",
        "\n",
        "df_surprise_undersampled = resample(df_surprise,\n",
        "                                    replace=False,    # sample without replacement\n",
        "                                    n_samples=500,     # to match minority class\n",
        "                                    random_state=42) # reproducible results\n",
        "\n",
        "\n",
        "# Concatenate minority class with undersampled majority class\n",
        "df_balanced = pd.concat([df_majority_undersampled, df_sadness_undersampled, df_anger_undersampled,\n",
        "                         df_fear_undersampled, df_love_undersampled, df_surprise_undersampled])\n",
        "\n",
        "df = df_balanced\n",
        "display(df['label_name'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 4. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Now that our data is balanced, we visualize it to gain more insights.\n",
        "\n",
        "1. **Frequency Plot**: A horizontal bar chart visually confirms that all our classes now have an equal number of samples (500 each).\n",
        "2. **Words per Tweet**: We create a new feature, `Words per Tweet`, and use a boxplot to see if the length of a tweet varies across different emotions. This can reveal interesting patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": ["import matplotlib.pyplot as plt"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "label_counts = df['label_name'].value_counts(ascending=True)\n",
        "label_counts.plot.barh()\n",
        "plt.title(\"Frequency of Classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Words per Tweet'] = df['text'].str.split().apply(len)\n",
        "df.boxplot(\"Words per Tweet\", by=\"label_name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  tokenize 5. Text Tokenization\n",
        "\n",
        "Transformer models like BERT don't understand raw text. They require a numerical representation. **Tokenization** is the process of converting text into numbers the model can process.\n",
        "\n",
        "- **Model Checkpoint**: We select `answerdotai/ModernBERT-base`, a modern variant of the BERT model.\n",
        "- **`AutoTokenizer`**: We load the specific tokenizer that corresponds to our chosen model. This is crucial for ensuring the text is processed in the same way the model was pre-trained.\n",
        "- **Encoding**: The tokenizer converts our text into `input_ids` (numerical representations of words/subwords) and an `attention_mask` (which tells the model which tokens to pay attention to)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# model_ckpt = \"bert-base-uncased\"\n",
        "model_ckpt = \"answerdotai/ModernBERT-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "\n",
        "text = \"I love machine learning! Tokenization is awesome!!\"\n",
        "encoded_text = tokenizer(text)\n",
        "print(encoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(tokenizer.vocab), tokenizer.vocab_size, tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÇÔ∏è 6. Data Splitting and Formatting\n",
        "\n",
        "To properly train and evaluate our model, we split the data into three sets:\n",
        "- **Training Set**: Used to fine-tune the model (70% of the data).\n",
        "- **Validation Set**: Used to monitor performance during training and prevent overfitting (10%).\n",
        "- **Test Set**: A completely unseen set used for final model evaluation (20%).\n",
        "\n",
        "We use `stratify=df['label_name']` to ensure that each set has the same proportion of emotion classes. Finally, we convert these pandas DataFrames into a `DatasetDict`, the standard format for the Hugging Face `Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.3, stratify=df['label_name'])\n",
        "test, validation = train_test_split(test, test_size=1/3, stratify=test['label_name'])\n",
        "\n",
        "train.shape, test.shape, validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "dataset = DatasetDict(\n",
        "    {'train':Dataset.from_pandas(train, preserve_index=False),\n",
        "     'test':Dataset.from_pandas(test, preserve_index=False),\n",
        "     'validation': Dataset.from_pandas(validation, preserve_index=False)\n",
        "     }\n",
        "\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["dataset['train'][0], dataset['train'][1]"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üó∫Ô∏è 7. Applying Tokenization to the Entire Dataset\n",
        "\n",
        "We now apply our tokenizer to all splits of the `DatasetDict`. We define a `tokenize` function and use the efficient `.map()` method to apply it to every example.\n",
        "\n",
        "- **`batched=True`**: Processes multiple rows at once for speed.\n",
        "- **`padding=True`**: Adds special `[PAD]` tokens to shorter sentences so all sentences in a batch have the same length.\n",
        "- **`truncation=True`**: Cuts longer sentences down to the model's maximum acceptable length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    temp = tokenizer(batch['text'], padding=True, truncation=True)\n",
        "    return temp\n",
        "\n",
        "print(tokenize(dataset['train'][:2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üè∑Ô∏è 8. Model Configuration\n",
        "\n",
        "Before loading the model, we need to configure it for our specific task: multi-class sequence classification.\n",
        "\n",
        "- **`label2id` & `id2label`**: We create dictionaries to map our string labels (e.g., 'sadness') to integer IDs (e.g., `1`) and vice-versa. The model works with integers, but we want human-readable outputs.\n",
        "- **`AutoConfig`**: We load the model's configuration and update it with our number of labels and the mapping dictionaries.\n",
        "- **`AutoModelForSequenceClassification`**: We load the ModernBERT model with a new, untrained classification head on top. This head will be fine-tuned on our data.\n",
        "- **`device`**: We ensure the model is moved to a GPU (`cuda`) if available, for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# label2id, id2label\n",
        "label2id = {x['label_name']:x['label'] for x in dataset['train']}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "label2id, id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "import torch\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_ckpt, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "num_labels = len(label2id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = AutoConfig.from_pretrained(model_ckpt, label2id=label2id, id2label=id2label)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# !pip install evaluate"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 9. Training Setup\n",
        "\n",
        "We use the Hugging Face `Trainer` API to handle the training loop. This requires two main components:\n",
        "\n",
        "1. **`TrainingArguments`**: This object defines all the hyperparameters for the training run, such as:\n",
        "   - `num_train_epochs`: The number of times to iterate over the full training dataset.\n",
        "   - `learning_rate`: Controls how much the model's weights are adjusted during training.\n",
        "   - `per_device_train_batch_size`: The number of samples processed at once.\n",
        "   - `eval_strategy = 'epoch'`: Tells the trainer to run an evaluation at the end of each epoch.\n",
        "\n",
        "2. **`compute_metrics` function**: A custom function to calculate performance metrics during evaluation. We use `accuracy` and the weighted `f1-score`, which is a robust metric for imbalanced (or in our case, multi-class) datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "training_dir = \"bert_base_train_dir\"\n",
        "\n",
        "training_args = TrainingArguments( output_dir=training_dir,\n",
        "                                  overwrite_output_dir = True,\n",
        "                                  num_train_epochs = 15,\n",
        "                                  learning_rate = 2e-5,\n",
        "                                  per_device_train_batch_size = batch_size,\n",
        "                                  per_device_eval_batch_size = batch_size,\n",
        "                                  weight_decay = 0.05,\n",
        "                                  eval_strategy = 'epoch',\n",
        "                                  disable_tqdm = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use sklearn to build compute metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ñ∂Ô∏è 10. Model Training\n",
        "\n",
        "With all the components prepared (model, tokenizer, datasets, arguments, metrics), we instantiate the `Trainer` object. This powerful class abstracts away the entire training and evaluation loop.\n",
        "\n",
        "Calling `trainer.train()` kicks off the fine-tuning process. The trainer will iterate through the training data, update the model's weights, and periodically evaluate its performance on the validation set, printing the results after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args,\n",
        "                  compute_metrics=compute_metrics,\n",
        "                  train_dataset = emotion_encoded['train'],\n",
        "                  eval_dataset = emotion_encoded['validation'],\n",
        "                  tokenizer = tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["trainer.train()\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßê 11. Final Evaluation\n",
        "\n",
        "After training is complete, we perform a final evaluation on the held-out test set‚Äîdata the model has never seen before. \n",
        "\n",
        "- **`trainer.predict()`**: Generates predictions for the test set.\n",
        "- **`classification_report`**: Provides a detailed breakdown of performance for each class, including precision, recall, and F1-score.\n",
        "- **Confusion Matrix**: We then plot a confusion matrix using `seaborn`. This heatmap visualizes the model's predictions, showing exactly which classes are being predicted correctly and which ones are being confused for others. The diagonal represents correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds_output = trainer.predict(emotion_encoded['test'])\n",
        "preds_output.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
        "y_true = emotion_encoded['test'][:]['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot confusion matrix\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm, annot=True, xticklabels=label2id.keys(), yticklabels=label2id.keys(), fmt='d', cbar=False, cmap='Reds')\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 12. Inference and Deployment\n",
        "\n",
        "The final step is to use our trained model and save it for future applications.\n",
        "\n",
        "- **Inference Function**: We create a helper function `get_prediction` that takes raw text, tokenizes it, feeds it to the model, and returns a human-readable emotion label.\n",
        "- **Saving the Model**: `trainer.save_model()` serializes the fine-tuned model's weights and configuration to a directory.\n",
        "- **Using a `pipeline`**: For easy deployment, we load the saved model into a `text-classification` `pipeline`. This is the simplest way to use a Hugging Face model for inference, as it handles all the preprocessing and post-processing steps under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"I am super happy today. I got it done. Finally!!\"\n",
        "\n",
        "def get_prediction(text):\n",
        "    input_encoded = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**input_encoded)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    pred = torch.argmax(logits, dim=1).item()\n",
        "    return id2label[pred]\n",
        "\n",
        "get_prediction(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"Modern-bert-uncased-sentiment-model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use pipeline for prediciton\n",
        "from transformers import pipeline\n",
        "# bert-base-uncased-sentiment-model\n",
        "classifier = pipeline('text-classification', model= 'Modern-bert-uncased-sentiment-model')\n",
        "\n",
        "classifier([text, 'hello, how are you?', \"love you\", \"i am feeling low\",\" i love you\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
